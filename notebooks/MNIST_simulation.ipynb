{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ðŸ§  MNIST Demo Simulation with Cosmic Emanator TFNP Layer\n",
    "\n",
    "This notebook demonstrates a comparative training simulation between a standard linear layer and the Cosmic Emanator's TFNP Layer. We use MNIST dataset (28x28 grayscale images) to benchmark the performance in accuracy, stability, and training efficiency."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import math"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸš€ TFNPLayer Definition\n",
    "Layer enhanced by cosmic-inspired geometric modulations (torus and flower-inspired transformations)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class TFNPLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, phi=(1 + math.sqrt(5)) / 2):\n",
    "        super(TFNPLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.phi = phi\n",
    "        self.torus_radius = nn.Parameter(torch.tensor(1.0))\n",
    "        self.circle_radius = nn.Parameter(torch.tensor(0.5))\n",
    "        self.sin_term = torch.tensor(math.sin(math.pi / 6))\n",
    "\n",
    "    def forward(self, x):\n",
    "        linear_out = self.linear(x)\n",
    "        torus_factor = self.torus_radius * torch.cos(2 * math.pi * linear_out / self.phi)\n",
    "        flower_factor = self.circle_radius * (torch.sin(3 * math.pi * linear_out) + self.sin_term)\n",
    "        return F.relu(linear_out + torus_factor + flower_factor)"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ“¦ Data Loader for MNIST"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ“ˆ Simple Neural Network Definition"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, use_tfnp=False):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        if use_tfnp:\n",
    "            self.layer1 = TFNPLayer(784, 128)\n",
    "        else:\n",
    "            self.layer1 = nn.Linear(784, 128)\n",
    "        self.layer2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ›  Training and Evaluation Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def train(model, epochs=3):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    start_time = time.time()\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        losses.append(avg_loss)\n",
    "    training_time = time.time() - start_time\n",
    "    return losses, training_time\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "    return correct / len(test_loader.dataset)"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": ["## ðŸ”¥ Run the Simulations and Compare Results"],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Standard Net\n",
    "standard_net = SimpleNet(use_tfnp=False)\n",
    "standard_losses, standard_time = train(standard_net)\n",
    "standard_acc = evaluate(standard_net)\n",
    "\n",
    "# TFNP Net\n",
    "tfnp_net = SimpleNet(use_tfnp=True)\n",
    "tfnp_losses, tfnp_time = train(tfnp_net)\n",
    "tfnp_acc = evaluate(tfnp_net)\n",
    "\n",
    "print(\"Standard Losses:\", standard_losses)\n",
    "print(\"Standard Training Time:\", standard_time)\n",
    "print(\"Standard Accuracy:\", standard_acc)\n",
    "print(\"TFNP Losses:\", tfnp_losses)\n",
    "print(\"TFNP Training Time:\", tfnp_time)\n",
    "print(\"TFNP Accuracy:\", tfnp_acc)"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "name": "python3"},
  "language_info": {"version": "3.11", "name": "python"}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
