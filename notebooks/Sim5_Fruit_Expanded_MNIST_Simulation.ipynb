{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83d\udd22 MNIST Classification: Standard Linear vs TFNP Layer\n", "\n", "This notebook demonstrates training performance on the MNIST digit classification task using a standard linear neural network versus the Cosmic Emanator's TFNPLayer. We evaluate training stability, convergence speed, and accuracy to highlight the advantages introduced by geometric modulations inspired by the Flower of Life and Fibonacci spirals."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import torch.nn as nn\n", "import torch.optim as optim\n", "from torchvision import datasets, transforms\n", "from torch.utils.data import DataLoader\n", "import time\n", "import numpy as np\n", "import torch.nn.functional as F\n", "import math\n", "\n", "# TFNP Layer Definition\n", "class TFNPLayer(nn.Module):\n", "    def __init__(self, in_features, out_features, phi=(1 + math.sqrt(5)) / 2):\n", "        super(TFNPLayer, self).__init__()\n", "        self.linear = nn.Linear(in_features, out_features)\n", "        self.phi = phi\n", "        self.torus_radius = nn.Parameter(torch.tensor(1.0))\n", "        self.circle_radius = nn.Parameter(torch.tensor(0.5))\n", "        self.sin_term = torch.tensor(math.sin(math.pi / 6))\n", "\n", "    def forward(self, x):\n", "        linear_out = self.linear(x)\n", "        torus_factor = self.torus_radius * torch.cos(2 * math.pi * linear_out / self.phi)\n", "        flower_factor = self.circle_radius * (torch.sin(3 * math.pi * linear_out) + self.sin_term)\n", "        return F.relu(linear_out + torus_factor + flower_factor)\n", "\n", "# Simple Network Definition\n", "class SimpleNet(nn.Module):\n", "    def __init__(self, use_tfnp=False):\n", "        super(SimpleNet, self).__init__()\n", "        if use_tfnp:\n", "            self.layer1 = TFNPLayer(784, 128)\n", "        else:\n", "            self.layer1 = nn.Linear(784, 128)\n", "        self.layer2 = nn.Linear(128, 10)\n", "\n", "    def forward(self, x):\n", "        x = x.view(-1, 784)\n", "        x = F.relu(self.layer1(x))\n", "        x = self.layer2(x)\n", "        return x\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### \ud83d\udce5 Loading MNIST Data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["transform = transforms.Compose([\n", "    transforms.ToTensor(),\n", "    transforms.Normalize((0.1307,), (0.3081,))\n", "])\n", "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n", "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n", "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n", "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### \ud83d\ude80 Training and Evaluation Functions"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train(model, epochs=3):\n", "    criterion = nn.CrossEntropyLoss()\n", "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n", "    start_time = time.time()\n", "    losses = []\n", "    for epoch in range(epochs):\n", "        model.train()\n", "        total_loss = 0\n", "        for data, target in train_loader:\n", "            optimizer.zero_grad()\n", "            output = model(data)\n", "            loss = criterion(output, target)\n", "            loss.backward()\n", "            optimizer.step()\n", "            total_loss += loss.item()\n", "        avg_loss = total_loss / len(train_loader)\n", "        losses.append(avg_loss)\n", "    end_time = time.time()\n", "    return losses, end_time - start_time\n", "\n", "def evaluate(model):\n", "    model.eval()\n", "    correct = 0\n", "    with torch.no_grad():\n", "        for data, target in test_loader:\n", "            output = model(data)\n", "            pred = output.argmax(dim=1, keepdim=True)\n", "            correct += pred.eq(target.view_as(pred)).sum().item()\n", "    accuracy = correct / len(test_loader.dataset)\n", "    return accuracy"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### \ud83e\uddea Running Simulations"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["standard_net = SimpleNet(use_tfnp=False)\n", "standard_losses, standard_time = train(standard_net)\n", "standard_acc = evaluate(standard_net)\n", "\n", "tfnp_net = SimpleNet(use_tfnp=True)\n", "tfnp_losses, tfnp_time = train(tfnp_net)\n", "tfnp_acc = evaluate(tfnp_net)\n", "\n", "print(f\"Standard Losses: {standard_losses}, Accuracy: {standard_acc:.4f}\")\n", "print(f\"TFNP Losses: {tfnp_losses}, Accuracy: {tfnp_acc:.4f}\")\n", "print(f\"Speedup: {standard_time / tfnp_time:.2f}x\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.11"}}, "nbformat": 4, "nbformat_minor": 5}
